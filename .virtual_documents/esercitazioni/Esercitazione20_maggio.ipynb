

















import numpy as np
import matplotlib.pyplot as plt
import random
seed=11
random.seed(seed)
np.random.seed(seed)

def relu(x):
  """

  Argomenti:
    x: Valore di input.

  Restituisce:
    Valore di output della funzione ReLU.
  """
  return np.where(x >= 0, x, 0)

def relu_derivative(x):
  """
  Funzione per calcolare la derivata vettoriale della funzione ReLU.

  Argomenti:
    x: Array di input.

  Restituisce:
    Array contenente la derivata della funzione ReLU per ogni elemento in x.
  """
  return np.where(x >= 0, 1, 0)






#integrare in vecchio codice scritto l'altra volta

def forward_propagation(x,w,b):
    a1=w[0]*x+b[0]
    z1=relu(a1)

    a2=w[1]*z1+b[1]
    z2=relu(a2)

    a3=w[2]*z2+b[2]
    z3=relu(a3)

    return z3,a1,a2,a3,z1,z2

def backward_propagation(y_pred,y,w,b,a1,a2,a3,z1,z2,x,learning_rate):
    delta_3=(y_pred-y)*relu_derivative(a3)
    delta_2=delta_3*w[2]*relu_derivative(a2)
    delta_1=delta_2*w[1]*relu_derivative(a1)

    #Aggiornamento dei pesi
    w[2]=w[2]-learning_rate*np.sum(delta_3*z2)/nT
    w[1]=w[1]-learning_rate*np.sum(delta_2*z1)/nT
    w[0]=w[0]-learning_rate*np.sum(delta_1*x)/nT

    #Aggiornamento bias
    b[2]=b[2]-learning_rate*np.sum(delta_3)/nT
    b[1]=b[1]-learning_rate*np.sum(delta_2)/nT
    b[0]=b[0]-learning_rate*np.sum(delta_1)/nT

    return w,b


w=np.random.randn(3)
b=np.random.randn(3)


nT=x.shape[0]
f_loss_1=[]

for epoch in range(epochs):
    #Predizione
    y_pred,a1,a2,a3,z1,z2=forward_propagation(x,w,b)
    #Calcola la funzione costo
    loss=mse(y_pred,y)
    f_loss_1.append(loss)
    w,b=backward_propagation(y_pred,y,w,b,a1,a2,a3,z1,z2,x,learning_rate,nT)

#Predizione sui nuovi dati
xnew=np.linspace(0.0,10,100)
y_pred_new,a1,a2,a3,z1,z2=forward_propagation(xnew,w,b)


plt.plot(x,y,xnew,y_pred_new,'r-')
plt.show()
plt.semilogy(f_loss_1)


















